{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "from pyEDM import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "    message=\"A worker stopped while some jobs were given to the executor.\",\n",
    "    module=\"joblib.externals.loky.process_executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(data, num_lags=1, tau=1):\n",
    "    ''' Get a dataframe with all the possible valid lags of the variables. '''\n",
    "    \n",
    "    backward_lags = pd.concat([data[var].shift(lag*tau).rename(f'{var}(t-{lag*tau})') for lag in range(num_lags+1) for var in data.columns], axis=1)\n",
    "    forward_lags  = pd.concat([data[var].shift(-1*lag*tau).rename(f'{var}(t+{lag*tau})') for lag in range(1,num_lags+1) for var in data.columns], axis=1)\n",
    "    block = pd.concat([backward_lags, forward_lags], axis=1)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xmap_results_smap(block, target, embeddings, Tp, theta, lib, pred):\n",
    "    '''Function to do exhaustive search of embeddings.'''\n",
    "    \n",
    "    def compute_rho(block, target, embedding, Tp, theta, lib, pred):\n",
    "        xmap = SMap(dataFrame=block, target=target, columns=embedding, Tp=Tp, theta=theta, embedded=True, lib=lib, pred=pred, noTime=True)\n",
    "        rho = xmap['predictions'][['Observations', 'Predictions']].corr().iloc[0,1]\n",
    "        return embedding, xmap['predictions'], rho\n",
    "\n",
    "    xmap_results = pd.DataFrame(columns=['embedding', 'rho'])\n",
    "    xmap_results = Parallel(n_jobs=-1)(delayed(compute_rho)(block, target, embedding, Tp, theta, lib, pred) for embedding in embeddings)\n",
    "    xmap_results = pd.DataFrame(xmap_results, columns=['embedding', 'result', 'rho'])\n",
    "    xmap_results = xmap_results.sort_values(by='rho', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return xmap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiview Cross-Mapping Function\n",
    "\n",
    "def MVCM(block, target, xmap_results, Tp, gap_radius, theta, lib, pred, E, k, self_weight):\n",
    "    \n",
    "    # Get lib and pred indices, adjusted to match pyEDM\n",
    "    lib_start, lib_end = map(int, lib.split())\n",
    "    pred_start, pred_end = map(int, pred.split())\n",
    "    lib_start -= 1; lib_end -= 1\n",
    "    pred_start -= 1; pred_end -= 1\n",
    "    \n",
    "    if Tp > 0:\n",
    "        pred_end += Tp\n",
    "    elif Tp < 0:\n",
    "        pred_start -= -1 * Tp\n",
    "    \n",
    "    # If k > number of system views, return NaNs as the filtered timeseries\n",
    "    if k > len(xmap_results):\n",
    "        filtered_timeseries = pd.DataFrame([np.nan] * len(xmap_results.loc[0,'result']['Predictions']))\n",
    "        return filtered_timeseries\n",
    "    \n",
    "    filter_input = pd.DataFrame()\n",
    "    filter_input = pd.concat([xmap_results.loc[i,'result']['Predictions'] for i in range(0,k)], axis=1)\n",
    "    filter_input.index = block.loc[pred_start:pred_end,:].index\n",
    "    \n",
    "    self = block.loc[pred_start:pred_end,f'{target}(t-0)']\n",
    "    self.index = range(pred_start,pred_end+1)\n",
    "    filter_input['self'] = self\n",
    "    filter_input['vals_to_avg'] = filter_input.apply(lambda row: row.tolist(), axis=1)\n",
    "    \n",
    "    # Get weights based on cross-map skill of embeddings\n",
    "    weights = xmap_results.loc[:k-1,'rho'].tolist()\n",
    "    weights = [x if x >= 0 else 0 for x in weights]                  # Make negative weights 0\n",
    "    \n",
    "    if np.sum(weights) > 0:\n",
    "        weights = [(1 - self_weight/100)*(weight/np.sum(weights)) for weight in weights]\n",
    "    else:\n",
    "        weights = [(1 - self_weight/100)*(1/len(weights)) for weight in weights]\n",
    "    \n",
    "    weights = weights + [self_weight/100]\n",
    "    filter_input['weights'] = [weights] * len(filter_input)\n",
    "    \n",
    "    # Get filtered values\n",
    "    vals_to_avg = np.array(filter_input['vals_to_avg'].tolist())\n",
    "    weights = np.array(filter_input['weights'].tolist())\n",
    "\n",
    "    filter_input['filtered_points'] = np.nansum(vals_to_avg * weights, axis=1)\n",
    "    \n",
    "    filtered_timeseries = filter_input[['filtered_points']].copy()\n",
    "    \n",
    "    # Make sure filtered values are positive\n",
    "    filtered_timeseries[filtered_timeseries<0] = 0\n",
    "    \n",
    "    return filtered_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_MVCM(block, target, all_xmap_results, Tp, gap_radius, lib, pred, E_list, k_list, theta_list):\n",
    "    \n",
    "    # Get lib and pred indices, adjusted to match pyEDM\n",
    "    lib_start, lib_end = map(int, lib.split())\n",
    "    pred_start, pred_end = map(int, pred.split())\n",
    "    lib_start -= 1; lib_end -= 1\n",
    "    pred_start -= 1; pred_end -= 1\n",
    "    \n",
    "    # Optimize parameters using a self_weight of 0 until a self_weight is chosen at the end\n",
    "    self_weight = 0\n",
    "    \n",
    "    # Choose the E, k, and theta that give the best multiview cross-map prediction of the observed data with a self_weight of 0\n",
    "\n",
    "    xmap_results_dict = {}\n",
    "    \n",
    "    # Get multiview cross-map predictions for E, k, and theta combinations\n",
    "    mvcm_results = pd.DataFrame(columns=['E', 'k', 'theta', 'rho', 'xmap_results', 'noisy_and_filtered'])\n",
    "    \n",
    "    total_iterations = len(list(product(E_list, theta_list, k_list)))\n",
    "    #with tqdm(total=total_iterations) as pbar:\n",
    "    for E, theta in product(E_list, theta_list):\n",
    "\n",
    "        # Get random embeddings and their cross-map skill\n",
    "        xmap_results = {k: v for k, v in all_xmap_results.items() if (k.split('_')[0] == target) & \n",
    "                                                            (k.split('_')[1] == lib) &\n",
    "                                                            (k.split('_')[2] == str(E)) &\n",
    "                                                            (k.split('_')[3] == str(theta))}\n",
    "        key = list(xmap_results.keys())[0]\n",
    "        xmap_results = xmap_results[key]\n",
    "        xmap_results_dict['{0}_{1}'.format(E, theta)] = xmap_results\n",
    "\n",
    "        # Get multiview cross-map predictions for ks in k_list \n",
    "        for k in k_list:\n",
    "\n",
    "            filtered = MVCM(block, target, xmap_results_dict[f'{str(E)}_{str(theta)}'], Tp, gap_radius, theta, lib, pred, E, k, self_weight)\n",
    "\n",
    "            # Align indices of noisy target with indices of filtered_timeseries\n",
    "            noisy_target = block.loc[pred_start:pred_end,f'{target}(t-0)']\n",
    "            noisy_and_filtered = pd.concat([noisy_target, filtered], axis=1)\n",
    "            noisy_and_filtered.columns = [f'noisy_{target}', f'filtered_{target}']\n",
    "            rho = noisy_and_filtered.corr().iloc[0,1]\n",
    "            mvcm_results.loc[len(mvcm_results)] = [E, k, theta, rho, xmap_results, noisy_and_filtered]\n",
    "                #pbar.update(1)\n",
    "\n",
    "    mvcm_results = mvcm_results.sort_values(by='rho', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    E = int(mvcm_results.loc[0,'E'])\n",
    "    k = int(mvcm_results.loc[0,'k'])\n",
    "    theta = int(mvcm_results.loc[0,'theta'])\n",
    "    \n",
    "    return E, k, theta, mvcm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APD_46025(t-0)</th>\n",
       "      <th>ATMP_46025(t-0)</th>\n",
       "      <th>DEWP_46025(t-0)</th>\n",
       "      <th>DPD_46025(t-0)</th>\n",
       "      <th>GST_46025(t-0)</th>\n",
       "      <th>MWD_46025(t-0)</th>\n",
       "      <th>PRES_46025(t-0)</th>\n",
       "      <th>TIDE_46025(t-0)</th>\n",
       "      <th>VIS_46025(t-0)</th>\n",
       "      <th>WDIR_46025(t-0)</th>\n",
       "      <th>...</th>\n",
       "      <th>DPD_46025(t+50)</th>\n",
       "      <th>GST_46025(t+50)</th>\n",
       "      <th>MWD_46025(t+50)</th>\n",
       "      <th>PRES_46025(t+50)</th>\n",
       "      <th>TIDE_46025(t+50)</th>\n",
       "      <th>VIS_46025(t+50)</th>\n",
       "      <th>WDIR_46025(t+50)</th>\n",
       "      <th>WSPD_46025(t+50)</th>\n",
       "      <th>WTMP_46025(t+50)</th>\n",
       "      <th>WVHT_46025(t+50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.497083</td>\n",
       "      <td>18.204167</td>\n",
       "      <td>14.841667</td>\n",
       "      <td>14.572917</td>\n",
       "      <td>4.354167</td>\n",
       "      <td>192.500000</td>\n",
       "      <td>1009.750000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>282.916667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.940833</td>\n",
       "      <td>3.354167</td>\n",
       "      <td>212.333333</td>\n",
       "      <td>1021.220833</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>155.333333</td>\n",
       "      <td>2.712500</td>\n",
       "      <td>17.600000</td>\n",
       "      <td>0.707500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.457500</td>\n",
       "      <td>17.858333</td>\n",
       "      <td>15.537500</td>\n",
       "      <td>13.681250</td>\n",
       "      <td>3.316667</td>\n",
       "      <td>196.416667</td>\n",
       "      <td>1011.145833</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>173.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>11.592500</td>\n",
       "      <td>2.862500</td>\n",
       "      <td>216.083333</td>\n",
       "      <td>1020.445833</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>218.125000</td>\n",
       "      <td>2.200000</td>\n",
       "      <td>17.595833</td>\n",
       "      <td>0.907917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.745833</td>\n",
       "      <td>18.829167</td>\n",
       "      <td>15.804167</td>\n",
       "      <td>13.890417</td>\n",
       "      <td>3.291667</td>\n",
       "      <td>196.125000</td>\n",
       "      <td>1011.770833</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>283.458333</td>\n",
       "      <td>...</td>\n",
       "      <td>11.764167</td>\n",
       "      <td>6.129167</td>\n",
       "      <td>246.875000</td>\n",
       "      <td>1017.941667</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>260.833333</td>\n",
       "      <td>5.037500</td>\n",
       "      <td>18.050000</td>\n",
       "      <td>1.493333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.666667</td>\n",
       "      <td>17.587500</td>\n",
       "      <td>15.283333</td>\n",
       "      <td>12.932083</td>\n",
       "      <td>4.037500</td>\n",
       "      <td>194.500000</td>\n",
       "      <td>1013.425000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>255.541667</td>\n",
       "      <td>...</td>\n",
       "      <td>12.523750</td>\n",
       "      <td>5.900000</td>\n",
       "      <td>248.916667</td>\n",
       "      <td>1016.116667</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>222.291667</td>\n",
       "      <td>4.587500</td>\n",
       "      <td>18.058333</td>\n",
       "      <td>1.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.857917</td>\n",
       "      <td>18.595833</td>\n",
       "      <td>16.250000</td>\n",
       "      <td>8.708750</td>\n",
       "      <td>6.204167</td>\n",
       "      <td>247.416667</td>\n",
       "      <td>1012.225000</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>300.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>9.740417</td>\n",
       "      <td>9.387500</td>\n",
       "      <td>254.375000</td>\n",
       "      <td>1014.483333</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>279.708333</td>\n",
       "      <td>7.920833</td>\n",
       "      <td>18.479167</td>\n",
       "      <td>1.507083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439</th>\n",
       "      <td>6.469167</td>\n",
       "      <td>18.154167</td>\n",
       "      <td>10.450000</td>\n",
       "      <td>10.906250</td>\n",
       "      <td>2.941667</td>\n",
       "      <td>261.916667</td>\n",
       "      <td>1017.208333</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>175.375000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>6.915833</td>\n",
       "      <td>17.866667</td>\n",
       "      <td>52.050000</td>\n",
       "      <td>14.812083</td>\n",
       "      <td>3.683333</td>\n",
       "      <td>246.541667</td>\n",
       "      <td>1016.112500</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>223.291667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>6.949167</td>\n",
       "      <td>17.529167</td>\n",
       "      <td>12.337500</td>\n",
       "      <td>15.577917</td>\n",
       "      <td>3.629167</td>\n",
       "      <td>240.708333</td>\n",
       "      <td>1015.258333</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>253.833333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>9.140833</td>\n",
       "      <td>16.525000</td>\n",
       "      <td>13.254167</td>\n",
       "      <td>16.134583</td>\n",
       "      <td>4.437500</td>\n",
       "      <td>268.416667</td>\n",
       "      <td>1014.595833</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>235.708333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>14.314583</td>\n",
       "      <td>14.495833</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>18.055000</td>\n",
       "      <td>14.033333</td>\n",
       "      <td>300.625000</td>\n",
       "      <td>1009.537500</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>295.083333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>444 rows × 1313 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     APD_46025(t-0)  ATMP_46025(t-0)  DEWP_46025(t-0)  DPD_46025(t-0)  \\\n",
       "0          6.497083        18.204167        14.841667       14.572917   \n",
       "1          6.457500        17.858333        15.537500       13.681250   \n",
       "2          6.745833        18.829167        15.804167       13.890417   \n",
       "3          5.666667        17.587500        15.283333       12.932083   \n",
       "4          4.857917        18.595833        16.250000        8.708750   \n",
       "..              ...              ...              ...             ...   \n",
       "439        6.469167        18.154167        10.450000       10.906250   \n",
       "440        6.915833        17.866667        52.050000       14.812083   \n",
       "441        6.949167        17.529167        12.337500       15.577917   \n",
       "442        9.140833        16.525000        13.254167       16.134583   \n",
       "443       14.314583        14.495833         8.800000       18.055000   \n",
       "\n",
       "     GST_46025(t-0)  MWD_46025(t-0)  PRES_46025(t-0)  TIDE_46025(t-0)  \\\n",
       "0          4.354167      192.500000      1009.750000             99.0   \n",
       "1          3.316667      196.416667      1011.145833             99.0   \n",
       "2          3.291667      196.125000      1011.770833             99.0   \n",
       "3          4.037500      194.500000      1013.425000             99.0   \n",
       "4          6.204167      247.416667      1012.225000             99.0   \n",
       "..              ...             ...              ...              ...   \n",
       "439        2.941667      261.916667      1017.208333             99.0   \n",
       "440        3.683333      246.541667      1016.112500             99.0   \n",
       "441        3.629167      240.708333      1015.258333             99.0   \n",
       "442        4.437500      268.416667      1014.595833             99.0   \n",
       "443       14.033333      300.625000      1009.537500             99.0   \n",
       "\n",
       "     VIS_46025(t-0)  WDIR_46025(t-0)  ...  DPD_46025(t+50)  GST_46025(t+50)  \\\n",
       "0              99.0       282.916667  ...        12.940833         3.354167   \n",
       "1              99.0       173.375000  ...        11.592500         2.862500   \n",
       "2              99.0       283.458333  ...        11.764167         6.129167   \n",
       "3              99.0       255.541667  ...        12.523750         5.900000   \n",
       "4              99.0       300.750000  ...         9.740417         9.387500   \n",
       "..              ...              ...  ...              ...              ...   \n",
       "439            99.0       175.375000  ...              NaN              NaN   \n",
       "440            99.0       223.291667  ...              NaN              NaN   \n",
       "441            99.0       253.833333  ...              NaN              NaN   \n",
       "442            99.0       235.708333  ...              NaN              NaN   \n",
       "443            99.0       295.083333  ...              NaN              NaN   \n",
       "\n",
       "     MWD_46025(t+50)  PRES_46025(t+50)  TIDE_46025(t+50)  VIS_46025(t+50)  \\\n",
       "0         212.333333       1021.220833              99.0             99.0   \n",
       "1         216.083333       1020.445833              99.0             99.0   \n",
       "2         246.875000       1017.941667              99.0             99.0   \n",
       "3         248.916667       1016.116667              99.0             99.0   \n",
       "4         254.375000       1014.483333              99.0             99.0   \n",
       "..               ...               ...               ...              ...   \n",
       "439              NaN               NaN               NaN              NaN   \n",
       "440              NaN               NaN               NaN              NaN   \n",
       "441              NaN               NaN               NaN              NaN   \n",
       "442              NaN               NaN               NaN              NaN   \n",
       "443              NaN               NaN               NaN              NaN   \n",
       "\n",
       "     WDIR_46025(t+50)  WSPD_46025(t+50)  WTMP_46025(t+50)  WVHT_46025(t+50)  \n",
       "0          155.333333          2.712500         17.600000          0.707500  \n",
       "1          218.125000          2.200000         17.595833          0.907917  \n",
       "2          260.833333          5.037500         18.050000          1.493333  \n",
       "3          222.291667          4.587500         18.058333          1.585000  \n",
       "4          279.708333          7.920833         18.479167          1.507083  \n",
       "..                ...               ...               ...               ...  \n",
       "439               NaN               NaN               NaN               NaN  \n",
       "440               NaN               NaN               NaN               NaN  \n",
       "441               NaN               NaN               NaN               NaN  \n",
       "442               NaN               NaN               NaN               NaN  \n",
       "443               NaN               NaN               NaN               NaN  \n",
       "\n",
       "[444 rows x 1313 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_data = pd.read_csv('Data/wind_data_w_gaps.csv', index_col=0)#.iloc[304:612] RANGE w/o missing values\n",
    "wind_data = wind_data.drop(columns=['WDIR_lj','WSPD_lj','GST_lj','WVHT_lj','DPD_lj','APD_lj','MWD_lj','PRES_lj','ATMP_lj','WTMP_lj','DEWP_lj','VIS_lj','TIDE_lj'])\n",
    "#wind_data = wind_data.set_index('time')\n",
    "# Put columns in alphabetical order\n",
    "sorted_columns = sorted(wind_data.columns)\n",
    "wind_data = wind_data[sorted_columns]\n",
    "\n",
    "# Make indices integers and save mapping to dates\n",
    "#date_to_int_map = {i: date for i, date in enumerate(HAB_data.index)}\n",
    "#HAB_data.index = range(len(HAB_data))\n",
    "wind_data.index = wind_data.index.astype(int)\n",
    "\n",
    "target = 'WSPD_46025'\n",
    "\n",
    "#HAB_data = HAB_data.drop(['Nitrite_(uM)','Nitrate_(uM)'],axis=1)\n",
    "\n",
    "\n",
    "block = get_block(wind_data.iloc[254:698], num_lags=50, tau=1)\n",
    "block.index = np.arange(block.shape[0])\n",
    "block\n",
    "#HAB_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APD_46025     0\n",
      "ATMP_46025    0\n",
      "DEWP_46025    0\n",
      "DPD_46025     0\n",
      "GST_46025     0\n",
      "MWD_46025     0\n",
      "PRES_46025    0\n",
      "TIDE_46025    0\n",
      "VIS_46025     0\n",
      "WDIR_46025    0\n",
      "WSPD_46025    0\n",
      "WTMP_46025    0\n",
      "WVHT_46025    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wind_data.iloc[254:698].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(data, num_lags=1, tau=1):\n",
    "    ''' Get a dataframe with all the possible valid lags of the variables. '''\n",
    "    \n",
    "    backward_lags = pd.concat([data[var].shift(lag*tau).rename(f'{var}(t-{lag*tau})') for lag in range(num_lags+1) for var in data.columns], axis=1)\n",
    "    forward_lags  = pd.concat([data[var].shift(-1*lag*tau).rename(f'{var}(t+{lag*tau})') for lag in range(1,num_lags+1) for var in data.columns], axis=1)\n",
    "    block = pd.concat([backward_lags, forward_lags], axis=1)\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccm(interaction, block, E_list, tau_list, theta_list, Tp, sample=50, sig=0.05):\n",
    "    solver = Ridge(alpha=1.0)#TRYING DIFFERNT SOLVER TO ENSURE CONVERGENCE\n",
    "    print(interaction)\n",
    "    lib = f'1 {len(block)}'\n",
    "    \n",
    "    # Get dataframe with two species of interest\n",
    "    A = interaction[0]; B = interaction[1]\n",
    "    df = block[[f'{A}(t-0)', f'{B}(t-0)']]\n",
    "    \n",
    "    driver = f'{A}(t-0)'\n",
    "    \n",
    "    E_tau_theta_results = pd.DataFrame(columns = ['E', 'tau', 'theta', 'rho'])\n",
    "    for E, tau, theta in list(product(E_list, tau_list, theta_list)):\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho = c.corr().iloc[0,1]\n",
    "        E_tau_theta_results.loc[len(E_tau_theta_results)] = [E, tau, theta, rho]\n",
    "    E_tau_theta_results = E_tau_theta_results.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Assign E, tau, and theta to be the optimal E, tau, and theta\n",
    "    ccm_value = E_tau_theta_results['rho'].max()\n",
    "    E = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'E'].item())\n",
    "    tau = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'tau'].item())\n",
    "    theta = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'theta'].item())\n",
    "        \n",
    "    # Get convergence p-value\n",
    "    convergence_p_value = get_convergence_p_value(block, sample, A, B, E, Tp, tau, theta)\n",
    "\n",
    "    # Preparing Output\n",
    "    output = {\n",
    "        'target (driver)': A,\n",
    "        'lib (driven)': B,\n",
    "        'E': E,\n",
    "        'tau': tau,\n",
    "        'theta': theta,\n",
    "        'E_tau_theta_results': E_tau_theta_results,\n",
    "        'ccm_value': ccm_value,\n",
    "        'convergence_p_value': convergence_p_value,\n",
    "        'correlation': df.corr().iloc[0,1]\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_convergence_p_value(df, sample, A, B, E, Tp, tau, theta):\n",
    "    # Get convergence p-value for CCM (one-tailed t-test on cross-map values using 20% and 50% library sizes)\n",
    "    # H0: μ_20% ≥ μ_50%\n",
    "    # HA: μ_20% < μ_50%\n",
    "    # If p < 0.05, the 20% library size trials have a rho that is significantly smaller than the 50% library trials  \n",
    "    solver = Ridge(alpha=1.0)#TRYING DIFFERNT SOLVER TO ENSURE CONVERGENCE\n",
    "    libsize1 = int(np.ceil(df.shape[0]/5))   # 20% of the full library size\n",
    "    libsize2 = int(np.ceil(df.shape[0]/2))   # 50% of the full library size\n",
    "    \n",
    "    max_iterations = 10 * sample\n",
    "    \n",
    "    # Get list of rhos for libsize1\n",
    "    rhos1 = []; iteration_count = 0\n",
    "    while len(rhos1) < sample and iteration_count < max_iterations:\n",
    "        start = np.random.randint(libsize1, len(df))\n",
    "        library = [start - libsize1, start]\n",
    "        data_subset = df.iloc[library[0]:library[1]]\n",
    "        lib = f'{library[0]+1} {library[1]+1}'\n",
    "        driver = f'{A}(t-0)'\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho1 = c.corr().iloc[0,1]\n",
    "        if not np.isnan(rho1):\n",
    "            rhos1.append(rho1)\n",
    "        iteration_count += 1\n",
    "        \n",
    "    # Get list of rhos for libsize2\n",
    "    rhos2 = []; iteration_count = 0\n",
    "    while len(rhos2) < sample and iteration_count < max_iterations:\n",
    "        start = np.random.randint(libsize2, len(df))\n",
    "        library = [start - libsize2, start]\n",
    "        data_subset = df.iloc[library[0]:library[1]]\n",
    "        lib = f'{library[0]+1} {library[1]+1}'\n",
    "        driver = f'{A}(t-0)'\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho2 = c.corr().iloc[0,1]\n",
    "        if not np.isnan(rho2):\n",
    "            rhos2.append(rho2)\n",
    "        iteration_count += 1\n",
    "    \n",
    "    convergence_t_stat, convergence_p_value = ttest_ind(rhos1, rhos2, alternative='less')\n",
    "    \n",
    "    return convergence_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('APD_46025', 'WSPD_46025')\n",
      "('ATMP_46025', 'WSPD_46025')\n",
      "('DPD_46025', 'WSPD_46025')\n",
      "('GST_46025', 'WSPD_46025')\n",
      "('MWD_46025', 'WSPD_46025')\n",
      "('PRES_46025', 'WSPD_46025')\n",
      "('DEWP_46025', 'WSPD_46025')\n",
      "('TIDE_46025', 'WSPD_46025')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "can only convert an array of size 1 to a Python scalar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 463, in _process_worker\n    r = call_item()\n  File \"/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/externals/loky/process_executor.py\", line 291, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in __call__\n    return [func(*args, **kwargs)\n  File \"/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py\", line 598, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"/var/folders/5y/74w_zv855875xvjtmhp4g1xw0000gn/T/ipykernel_99780/2674098243.py\", line 24, in ccm\n  File \"/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/pandas/core/base.py\", line 418, in item\n    raise ValueError(\"can only convert an array of size 1 to a Python scalar\")\nValueError: can only convert an array of size 1 to a Python scalar\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m interaction \u001b[38;5;241m=\u001b[39m target_interactions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#ccm(target_interactions[2], block, E_list, tau_list, theta_list, Tp)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mccm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minteraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_interactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#CHaNGED TO FIRST 8 FOR SIMPLICITY\u001b[39;00m\n\u001b[1;32m     15\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:1754\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_retrieval():\n\u001b[1;32m   1748\u001b[0m \n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;66;03m# If the callback thread of a worker has signaled that its task\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m     \u001b[38;5;66;03m# triggered an exception, or if the retrieval loop has raised an\u001b[39;00m\n\u001b[1;32m   1751\u001b[0m     \u001b[38;5;66;03m# exception (e.g. `GeneratorExit`), exit the loop and surface the\u001b[39;00m\n\u001b[1;32m   1752\u001b[0m     \u001b[38;5;66;03m# worker traceback.\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aborting:\n\u001b[0;32m-> 1754\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_error_fast\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1755\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1757\u001b[0m     \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m     \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:1789\u001b[0m, in \u001b[0;36mParallel._raise_error_fast\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1785\u001b[0m \u001b[38;5;66;03m# If this error job exists, immediately raise the error by\u001b[39;00m\n\u001b[1;32m   1786\u001b[0m \u001b[38;5;66;03m# calling get_result. This job might not exists if abort has been\u001b[39;00m\n\u001b[1;32m   1787\u001b[0m \u001b[38;5;66;03m# called directly or if the generator is gc'ed.\u001b[39;00m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m error_job \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1789\u001b[0m     \u001b[43merror_job\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:745\u001b[0m, in \u001b[0;36mBatchCompletionCallBack.get_result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    739\u001b[0m backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparallel\u001b[38;5;241m.\u001b[39m_backend\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39msupports_retrieve_callback:\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;66;03m# We assume that the result has already been retrieved by the\u001b[39;00m\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;66;03m# callback thread, and is stored internally. It's just waiting to\u001b[39;00m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;66;03m# be returned.\u001b[39;00m\n\u001b[0;32m--> 745\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_return_or_raise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;66;03m# For other backends, the main thread needs to run the retrieval step.\u001b[39;00m\n\u001b[1;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:763\u001b[0m, in \u001b[0;36mBatchCompletionCallBack._return_or_raise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    762\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m==\u001b[39m TASK_ERROR:\n\u001b[0;32m--> 763\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    764\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n\u001b[1;32m    765\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: can only convert an array of size 1 to a Python scalar"
     ]
    }
   ],
   "source": [
    "E_list = range(2,13)\n",
    "tau_list = [-1,-2,-3]\n",
    "theta_list = [0,0.1,0.5,1,2,3,4,5,6,7,8,9]\n",
    "Tp = 0\n",
    "exclusion_radius = 0\n",
    "\n",
    "all_ccm_results = pd.DataFrame()\n",
    "interactions = list(permutations(wind_data.columns.tolist(),2))\n",
    "target_interactions = [pair for pair in interactions if target in pair]\n",
    "\n",
    "interaction = target_interactions[0]\n",
    "#ccm(target_interactions[2], block, E_list, tau_list, theta_list, Tp)\n",
    "results = Parallel(n_jobs=10)(\n",
    "    delayed(ccm)(interaction, block, E_list, tau_list, theta_list, Tp) for interaction in target_interactions[:8]) #CHaNGED TO FIRST 8 FOR SIMPLICITY\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get CCM results that show convergence (convergence p-value < 0.05)\u001b[39;00m\n\u001b[1;32m      3\u001b[0m ccm_cutoff \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.5\u001b[39m\n\u001b[0;32m----> 5\u001b[0m significant_results \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m[results_df\u001b[38;5;241m.\u001b[39mconvergence_p_value\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m0.05\u001b[39m]\n\u001b[1;32m      6\u001b[0m significant_results \u001b[38;5;241m=\u001b[39m significant_results\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mccm_value\u001b[39m\u001b[38;5;124m'\u001b[39m, ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m significant_results \u001b[38;5;241m=\u001b[39m significant_results[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget (driver)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlib (driven)\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mE\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtau\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtheta\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mccm_value\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39mreset_index(drop\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "# Get CCM results that show convergence (convergence p-value < 0.05)\n",
    "\n",
    "ccm_cutoff = 0.5\n",
    "\n",
    "significant_results = results_df[results_df.convergence_p_value<0.05]\n",
    "significant_results = significant_results.sort_values(by='ccm_value', ascending=False)\n",
    "significant_results = significant_results[['target (driver)', 'lib (driven)', 'E', 'tau', 'theta', 'ccm_value']].reset_index(drop=True)\n",
    "\n",
    "display(significant_results[significant_results.ccm_value>ccm_cutoff])\n",
    "\n",
    "# Choose system variables where the CCM value to or from the target is > ccm_cutoff\n",
    "system_variables = significant_results[significant_results.ccm_value > ccm_cutoff]\n",
    "system_variables = system_variables[['target (driver)', 'lib (driven)']].values.flatten().tolist()\n",
    "system_variables = list(set(system_variables))\n",
    "print('system variables: ')\n",
    "display(sorted(system_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phosphate_(uM)(t-0)', 'Phosphate_(uM)(t-3)', 'Phosphate_(uM)(t+3)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_lags_tau(block, target, Tp, tau, num_lags, exclusion_radius, system_variables):\n",
    "    \n",
    "    # Get lags of system variables\n",
    "    system_variable_lags = []\n",
    "    for var in system_variables:\n",
    "        # Get forwards and backwards lag of the system variables\n",
    "        var_backwards_lags = [f'{var}(t{i})' if i < 0 else f'{var}(t-{i})' for i in range(num_lags * tau, 1)]\n",
    "        var_backwards_lags = var_backwards_lags[::tau][:num_lags]\n",
    "        var_forwards_lags  = [f'{var}(t+{i})' for i in range(-(num_lags-1) * tau + 1)]\n",
    "        var_forwards_lags  = var_forwards_lags[::tau][:num_lags-1]\n",
    "        var_lags = var_backwards_lags + var_forwards_lags\n",
    "        system_variable_lags = system_variable_lags + var_lags\n",
    "    \n",
    "    # Remove (t-0) lag of target variable from valid_lags\n",
    "    valid_lags = [x for x in system_variable_lags if x != f'{target}(t-0)']\n",
    "\n",
    "    # If Tp = 0, remove [-exclusion_radius, exclusion_radius] lags of target variable from valid lags\n",
    "    if Tp == 0:\n",
    "        for r in range(-exclusion_radius, exclusion_radius+1):\n",
    "            if r < 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t{r})']\n",
    "            elif r == 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t-{r})']\n",
    "            elif r > 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t+{r})']\n",
    "                    \n",
    "    return valid_lags\n",
    "\n",
    "#target = 'Planktothrix_rubescens'\n",
    "system_variables = system_variables\n",
    "Tp = 0\n",
    "exclusion_radius = 6\n",
    "num_lags = 2   # Use -3, 0, and +3 lags of each variable\n",
    "tau = -3\n",
    "\n",
    "valid_lags = get_valid_lags_tau(block, target, Tp, tau, num_lags, exclusion_radius, system_variables)\n",
    "valid_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E = 1, # embeddings = 3\n",
      "E = 2, # embeddings = 3\n"
     ]
    }
   ],
   "source": [
    "random_embeddings = {}\n",
    "for E in range(1,3):\n",
    "    # Get random embeddings using valid lags\n",
    "    embeddings = set()\n",
    "    sample = 3#100000\n",
    "    max_trials = 5#10000000\n",
    "    trials = 0\n",
    "    while len(embeddings) < sample and trials < max_trials:\n",
    "        embedding = tuple(random.sample(valid_lags, E))\n",
    "        sorted_embedding = tuple(sorted(embedding))\n",
    "        if sorted_embedding not in embeddings:\n",
    "            embeddings.add(sorted_embedding)\n",
    "    trials += 1\n",
    "    embeddings = [list(embedding) for embedding in embeddings]\n",
    "    random_embeddings['{0}'.format((target, E, Tp, exclusion_radius))] = embeddings\n",
    "    print(f'E = {E}, # embeddings = {len(embeddings)}')\n",
    "    \n",
    "with open('random_embeddings_HAB.pkl', 'wb') as file:\n",
    "     pickle.dump(random_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"('Avg_Chloro_(mg/m3)', 1, 0, 0)\": [['Phosphate_(uM)(t+3)'],\n",
       "  ['Phosphate_(uM)(t-0)'],\n",
       "  ['Phosphate_(uM)(t-3)']],\n",
       " \"('Avg_Chloro_(mg/m3)', 2, 0, 0)\": [['Phosphate_(uM)(t-0)',\n",
       "   'Phosphate_(uM)(t-3)'],\n",
       "  ['Phosphate_(uM)(t+3)', 'Phosphate_(uM)(t-3)'],\n",
       "  ['Phosphate_(uM)(t+3)', 'Phosphate_(uM)(t-0)']]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HAB random embeddings\n",
    "with open('random_embeddings_HAB.pkl', 'rb') as file:\n",
    "    HAB_embeddings = pickle.load(file)\n",
    "HAB_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store xmap results\n",
    "folder = 'xmap results HAB 100000 random embeddings'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/147 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m key \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m HAB_embeddings\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m target \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m E \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m Tp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m exclusion_radius]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[0;32m---> 22\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HAB_embeddings[\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     24\u001b[0m xmap_results \u001b[38;5;241m=\u001b[39m get_xmap_results_smap(block, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(t-0)\u001b[39m\u001b[38;5;124m'\u001b[39m, embeddings, Tp, theta, lib, lib)\n\u001b[1;32m     26\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxmap_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Tp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_E_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_theta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Save HAB cross-mapping results\n",
    "\n",
    "E_list = range(4,25)\n",
    "theta_list = [1,5,9,15,25,35,45]\n",
    "Tp = 0\n",
    "exclusion_radius = 6\n",
    "self_weight = 0  # self_weight = 0 for gap filling\n",
    "lib = '1 832' #CHANGE TO LIBRARY SIZE\n",
    "pred = '1 832'\n",
    "\n",
    "total_iterations = len(E_list) * len(theta_list)\n",
    "\n",
    "gapfill_results = {}\n",
    "parameters = pd.DataFrame(columns=['target', 'noise_level', 'lib', 'pred', 'E', 'theta', 'k', 'rho'])\n",
    "block = get_block(HAB_data, num_lags=50)\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for E, theta in product(E_list, theta_list):\n",
    "\n",
    "        key = [key for key in HAB_embeddings.keys() if eval(key)[0] == target and eval(key)[1] == E and eval(key)[2] == Tp and eval(key)[3] == exclusion_radius]\n",
    "        print(key)\n",
    "        embeddings = HAB_embeddings[key[0]]\n",
    "\n",
    "        xmap_results = get_xmap_results_smap(block, f'{target}(t-0)', embeddings, Tp, theta, lib, lib)\n",
    "\n",
    "        file_path = os.path.join(folder, f'xmap_results_{target}_Tp_{Tp}_E_{E}_theta_{theta}.pkl')\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(xmap_results, f)\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([\"('Avg_Chloro_(mg/m3)', 1, 0, 0)\", \"('Avg_Chloro_(mg/m3)', 2, 0, 0)\"])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAB_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyedm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
