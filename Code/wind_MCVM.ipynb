{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:90% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from itertools import product\n",
    "from itertools import permutations\n",
    "from itertools import combinations\n",
    "from pyEDM import *\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:90% !important; }</style>'))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", \n",
    "    message=\"A worker stopped while some jobs were given to the executor.\",\n",
    "    module=\"joblib.externals.loky.process_executor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(data, num_lags=1, tau=1):\n",
    "    ''' Get a dataframe with all the possible valid lags of the variables. '''\n",
    "    \n",
    "    backward_lags = pd.concat([data[var].shift(lag*tau).rename(f'{var}(t-{lag*tau})') for lag in range(num_lags+1) for var in data.columns], axis=1)\n",
    "    forward_lags  = pd.concat([data[var].shift(-1*lag*tau).rename(f'{var}(t+{lag*tau})') for lag in range(1,num_lags+1) for var in data.columns], axis=1)\n",
    "    block = pd.concat([backward_lags, forward_lags], axis=1)\n",
    "\n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xmap_results_smap(block, target, embeddings, Tp, theta, lib, pred):\n",
    "    '''Function to do exhaustive search of embeddings.'''\n",
    "    \n",
    "    def compute_rho(block, target, embedding, Tp, theta, lib, pred):\n",
    "        xmap = SMap(dataFrame=block, target=target, columns=embedding, Tp=Tp, theta=theta, embedded=True, lib=lib, pred=pred, noTime=True)\n",
    "        rho = xmap['predictions'][['Observations', 'Predictions']].corr().iloc[0,1]\n",
    "        return embedding, xmap['predictions'], rho\n",
    "\n",
    "    xmap_results = pd.DataFrame(columns=['embedding', 'rho'])\n",
    "    xmap_results = Parallel(n_jobs=-1)(delayed(compute_rho)(block, target, embedding, Tp, theta, lib, pred) for embedding in embeddings)\n",
    "    xmap_results = pd.DataFrame(xmap_results, columns=['embedding', 'result', 'rho'])\n",
    "    xmap_results = xmap_results.sort_values(by='rho', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    return xmap_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiview Cross-Mapping Function\n",
    "\n",
    "def MVCM(block, target, xmap_results, Tp, gap_radius, theta, lib, pred, E, k, self_weight):\n",
    "    \n",
    "    # Get lib and pred indices, adjusted to match pyEDM\n",
    "    lib_start, lib_end = map(int, lib.split())\n",
    "    pred_start, pred_end = map(int, pred.split())\n",
    "    lib_start -= 1; lib_end -= 1\n",
    "    pred_start -= 1; pred_end -= 1\n",
    "    \n",
    "    if Tp > 0:\n",
    "        pred_end += Tp\n",
    "    elif Tp < 0:\n",
    "        pred_start -= -1 * Tp\n",
    "    \n",
    "    # If k > number of system views, return NaNs as the filtered timeseries\n",
    "    if k > len(xmap_results):\n",
    "        filtered_timeseries = pd.DataFrame([np.nan] * len(xmap_results.loc[0,'result']['Predictions']))\n",
    "        return filtered_timeseries\n",
    "    \n",
    "    filter_input = pd.DataFrame()\n",
    "    filter_input = pd.concat([xmap_results.loc[i,'result']['Predictions'] for i in range(0,k)], axis=1)\n",
    "    filter_input.index = block.loc[pred_start:pred_end,:].index\n",
    "    \n",
    "    self = block.loc[pred_start:pred_end,f'{target}(t-0)']\n",
    "    self.index = range(pred_start,pred_end+1)\n",
    "    filter_input['self'] = self\n",
    "    filter_input['vals_to_avg'] = filter_input.apply(lambda row: row.tolist(), axis=1)\n",
    "    \n",
    "    # Get weights based on cross-map skill of embeddings\n",
    "    weights = xmap_results.loc[:k-1,'rho'].tolist()\n",
    "    weights = [x if x >= 0 else 0 for x in weights]                  # Make negative weights 0\n",
    "    \n",
    "    if np.sum(weights) > 0:\n",
    "        weights = [(1 - self_weight/100)*(weight/np.sum(weights)) for weight in weights]\n",
    "    else:\n",
    "        weights = [(1 - self_weight/100)*(1/len(weights)) for weight in weights]\n",
    "    \n",
    "    weights = weights + [self_weight/100]\n",
    "    filter_input['weights'] = [weights] * len(filter_input)\n",
    "    \n",
    "    # Get filtered values\n",
    "    vals_to_avg = np.array(filter_input['vals_to_avg'].tolist())\n",
    "    weights = np.array(filter_input['weights'].tolist())\n",
    "\n",
    "    filter_input['filtered_points'] = np.nansum(vals_to_avg * weights, axis=1)\n",
    "    \n",
    "    filtered_timeseries = filter_input[['filtered_points']].copy()\n",
    "    \n",
    "    # Make sure filtered values are positive\n",
    "    filtered_timeseries[filtered_timeseries<0] = 0\n",
    "    \n",
    "    return filtered_timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_parameters_MVCM(block, target, all_xmap_results, Tp, gap_radius, lib, pred, E_list, k_list, theta_list):\n",
    "    \n",
    "    # Get lib and pred indices, adjusted to match pyEDM\n",
    "    lib_start, lib_end = map(int, lib.split())\n",
    "    pred_start, pred_end = map(int, pred.split())\n",
    "    lib_start -= 1; lib_end -= 1\n",
    "    pred_start -= 1; pred_end -= 1\n",
    "    \n",
    "    # Optimize parameters using a self_weight of 0 until a self_weight is chosen at the end\n",
    "    self_weight = 0\n",
    "    \n",
    "    # Choose the E, k, and theta that give the best multiview cross-map prediction of the observed data with a self_weight of 0\n",
    "\n",
    "    xmap_results_dict = {}\n",
    "    \n",
    "    # Get multiview cross-map predictions for E, k, and theta combinations\n",
    "    mvcm_results = pd.DataFrame(columns=['E', 'k', 'theta', 'rho', 'xmap_results', 'noisy_and_filtered'])\n",
    "    \n",
    "    total_iterations = len(list(product(E_list, theta_list, k_list)))\n",
    "    #with tqdm(total=total_iterations) as pbar:\n",
    "    for E, theta in product(E_list, theta_list):\n",
    "\n",
    "        # Get random embeddings and their cross-map skill\n",
    "        xmap_results = {k: v for k, v in all_xmap_results.items() if (k.split('_')[0] == target) & \n",
    "                                                            (k.split('_')[1] == lib) &\n",
    "                                                            (k.split('_')[2] == str(E)) &\n",
    "                                                            (k.split('_')[3] == str(theta))}\n",
    "        key = list(xmap_results.keys())[0]\n",
    "        xmap_results = xmap_results[key]\n",
    "        xmap_results_dict['{0}_{1}'.format(E, theta)] = xmap_results\n",
    "\n",
    "        # Get multiview cross-map predictions for ks in k_list \n",
    "        for k in k_list:\n",
    "\n",
    "            filtered = MVCM(block, target, xmap_results_dict[f'{str(E)}_{str(theta)}'], Tp, gap_radius, theta, lib, pred, E, k, self_weight)\n",
    "\n",
    "            # Align indices of noisy target with indices of filtered_timeseries\n",
    "            noisy_target = block.loc[pred_start:pred_end,f'{target}(t-0)']\n",
    "            noisy_and_filtered = pd.concat([noisy_target, filtered], axis=1)\n",
    "            noisy_and_filtered.columns = [f'noisy_{target}', f'filtered_{target}']\n",
    "            rho = noisy_and_filtered.corr().iloc[0,1]\n",
    "            mvcm_results.loc[len(mvcm_results)] = [E, k, theta, rho, xmap_results, noisy_and_filtered]\n",
    "                #pbar.update(1)\n",
    "\n",
    "    mvcm_results = mvcm_results.sort_values(by='rho', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    E = int(mvcm_results.loc[0,'E'])\n",
    "    k = int(mvcm_results.loc[0,'k'])\n",
    "    theta = int(mvcm_results.loc[0,'theta'])\n",
    "    \n",
    "    return E, k, theta, mvcm_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>APD_46025(t-0)</th>\n",
       "      <th>APD_lj(t-0)</th>\n",
       "      <th>ATMP_46025(t-0)</th>\n",
       "      <th>ATMP_lj(t-0)</th>\n",
       "      <th>DEWP_46025(t-0)</th>\n",
       "      <th>DEWP_lj(t-0)</th>\n",
       "      <th>DPD_46025(t-0)</th>\n",
       "      <th>DPD_lj(t-0)</th>\n",
       "      <th>GST_46025(t-0)</th>\n",
       "      <th>GST_lj(t-0)</th>\n",
       "      <th>...</th>\n",
       "      <th>VIS_46025(t+50)</th>\n",
       "      <th>VIS_lj(t+50)</th>\n",
       "      <th>WDIR_46025(t+50)</th>\n",
       "      <th>WDIR_lj(t+50)</th>\n",
       "      <th>WSPD_46025(t+50)</th>\n",
       "      <th>WSPD_lj(t+50)</th>\n",
       "      <th>WTMP_46025(t+50)</th>\n",
       "      <th>WTMP_lj(t+50)</th>\n",
       "      <th>WVHT_46025(t+50)</th>\n",
       "      <th>WVHT_lj(t+50)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.997917</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14.979167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>214.766667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.100000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.612500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>256.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.833333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.845833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.900833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.919583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>15.095833</td>\n",
       "      <td>NaN</td>\n",
       "      <td>48.883333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.515000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.441667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239.291667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.062500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.766667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.700833</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.115417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.825000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.858333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.334583</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.579167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>136.708333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.737500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.616667</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.988333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.558333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.883333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.639167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.850000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>211.208333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.483333</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.587500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.518333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.562083</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.700000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.716667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.494167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8.475000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.916667</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.554167</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13.675000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.933333</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5839</th>\n",
       "      <td>69.021250</td>\n",
       "      <td>8.341250</td>\n",
       "      <td>15.438889</td>\n",
       "      <td>999.0</td>\n",
       "      <td>13.327778</td>\n",
       "      <td>999.0</td>\n",
       "      <td>70.774306</td>\n",
       "      <td>12.993333</td>\n",
       "      <td>4.049306</td>\n",
       "      <td>2.400000</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5840</th>\n",
       "      <td>69.448403</td>\n",
       "      <td>9.065417</td>\n",
       "      <td>15.145139</td>\n",
       "      <td>999.0</td>\n",
       "      <td>12.409722</td>\n",
       "      <td>999.0</td>\n",
       "      <td>70.883889</td>\n",
       "      <td>13.167083</td>\n",
       "      <td>4.158333</td>\n",
       "      <td>3.912500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5841</th>\n",
       "      <td>69.757778</td>\n",
       "      <td>8.688750</td>\n",
       "      <td>15.186111</td>\n",
       "      <td>999.0</td>\n",
       "      <td>13.581944</td>\n",
       "      <td>999.0</td>\n",
       "      <td>71.243264</td>\n",
       "      <td>13.820000</td>\n",
       "      <td>4.306944</td>\n",
       "      <td>2.737500</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842</th>\n",
       "      <td>69.660972</td>\n",
       "      <td>8.941250</td>\n",
       "      <td>14.953472</td>\n",
       "      <td>999.0</td>\n",
       "      <td>13.088889</td>\n",
       "      <td>999.0</td>\n",
       "      <td>71.349653</td>\n",
       "      <td>13.394583</td>\n",
       "      <td>6.879167</td>\n",
       "      <td>3.958333</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5843</th>\n",
       "      <td>69.257639</td>\n",
       "      <td>7.801667</td>\n",
       "      <td>15.296528</td>\n",
       "      <td>999.0</td>\n",
       "      <td>11.974306</td>\n",
       "      <td>999.0</td>\n",
       "      <td>70.939306</td>\n",
       "      <td>12.807083</td>\n",
       "      <td>5.836111</td>\n",
       "      <td>4.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5844 rows × 2626 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      APD_46025(t-0)  APD_lj(t-0)  ATMP_46025(t-0)  ATMP_lj(t-0)  \\\n",
       "time                                                               \n",
       "0           5.997917          NaN        14.979167           NaN   \n",
       "1           4.919583          NaN        15.095833           NaN   \n",
       "2           8.115417          NaN        13.825000           NaN   \n",
       "3           8.988333          NaN        13.558333           NaN   \n",
       "4           6.562083          NaN        13.700000           NaN   \n",
       "...              ...          ...              ...           ...   \n",
       "5839       69.021250     8.341250        15.438889         999.0   \n",
       "5840       69.448403     9.065417        15.145139         999.0   \n",
       "5841       69.757778     8.688750        15.186111         999.0   \n",
       "5842       69.660972     8.941250        14.953472         999.0   \n",
       "5843       69.257639     7.801667        15.296528         999.0   \n",
       "\n",
       "      DEWP_46025(t-0)  DEWP_lj(t-0)  DPD_46025(t-0)  DPD_lj(t-0)  \\\n",
       "time                                                               \n",
       "0          214.766667           NaN       13.100000          NaN   \n",
       "1           48.883333           NaN       12.515000          NaN   \n",
       "2           10.858333           NaN       13.334583          NaN   \n",
       "3           11.883333           NaN       13.639167          NaN   \n",
       "4           12.716667           NaN        9.494167          NaN   \n",
       "...               ...           ...             ...          ...   \n",
       "5839        13.327778         999.0       70.774306    12.993333   \n",
       "5840        12.409722         999.0       70.883889    13.167083   \n",
       "5841        13.581944         999.0       71.243264    13.820000   \n",
       "5842        13.088889         999.0       71.349653    13.394583   \n",
       "5843        11.974306         999.0       70.939306    12.807083   \n",
       "\n",
       "      GST_46025(t-0)  GST_lj(t-0)  ...  VIS_46025(t+50)  VIS_lj(t+50)  \\\n",
       "time                               ...                                  \n",
       "0           4.612500          NaN  ...             99.0           NaN   \n",
       "1           4.441667          NaN  ...             99.0           NaN   \n",
       "2           2.579167          NaN  ...             99.0           NaN   \n",
       "3           4.850000          NaN  ...             99.0           NaN   \n",
       "4           8.475000          NaN  ...             99.0           NaN   \n",
       "...              ...          ...  ...              ...           ...   \n",
       "5839        4.049306     2.400000  ...              NaN           NaN   \n",
       "5840        4.158333     3.912500  ...              NaN           NaN   \n",
       "5841        4.306944     2.737500  ...              NaN           NaN   \n",
       "5842        6.879167     3.958333  ...              NaN           NaN   \n",
       "5843        5.836111     4.666667  ...              NaN           NaN   \n",
       "\n",
       "      WDIR_46025(t+50)  WDIR_lj(t+50)  WSPD_46025(t+50)  WSPD_lj(t+50)  \\\n",
       "time                                                                     \n",
       "0           256.750000            NaN          3.833333            NaN   \n",
       "1           239.291667            NaN          6.062500            NaN   \n",
       "2           136.708333            NaN          8.200000            NaN   \n",
       "3           211.208333            NaN          6.483333            NaN   \n",
       "4           161.916667            NaN          5.554167            NaN   \n",
       "...                ...            ...               ...            ...   \n",
       "5839               NaN            NaN               NaN            NaN   \n",
       "5840               NaN            NaN               NaN            NaN   \n",
       "5841               NaN            NaN               NaN            NaN   \n",
       "5842               NaN            NaN               NaN            NaN   \n",
       "5843               NaN            NaN               NaN            NaN   \n",
       "\n",
       "      WTMP_46025(t+50)  WTMP_lj(t+50)  WVHT_46025(t+50)  WVHT_lj(t+50)  \n",
       "time                                                                    \n",
       "0            13.845833            NaN          4.900833            NaN  \n",
       "1            13.766667            NaN          1.700833            NaN  \n",
       "2            13.737500            NaN          1.616667            NaN  \n",
       "3            13.587500            NaN          1.518333            NaN  \n",
       "4            13.675000            NaN          1.933333            NaN  \n",
       "...                ...            ...               ...            ...  \n",
       "5839               NaN            NaN               NaN            NaN  \n",
       "5840               NaN            NaN               NaN            NaN  \n",
       "5841               NaN            NaN               NaN            NaN  \n",
       "5842               NaN            NaN               NaN            NaN  \n",
       "5843               NaN            NaN               NaN            NaN  \n",
       "\n",
       "[5844 rows x 2626 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wind_data = pd.read_csv('Data/wind_data_w_gaps.csv', index_col=0)#.iloc[304:612] RANGE w/o missing values\n",
    "#wind_data = wind_data.set_index('time')\n",
    "# Put columns in alphabetical order\n",
    "sorted_columns = sorted(wind_data.columns)\n",
    "wind_data = wind_data[sorted_columns]\n",
    "\n",
    "# Make indices integers and save mapping to dates\n",
    "#date_to_int_map = {i: date for i, date in enumerate(HAB_data.index)}\n",
    "#HAB_data.index = range(len(HAB_data))\n",
    "wind_data.index = wind_data.index.astype(int)\n",
    "\n",
    "target = 'WSPD_lj'\n",
    "\n",
    "#HAB_data = HAB_data.drop(['Nitrite_(uM)','Nitrate_(uM)'],axis=1)\n",
    "\n",
    "\n",
    "block = get_block(wind_data, num_lags=50, tau=1)\n",
    "block\n",
    "#HAB_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "APD_46025     227\n",
      "APD_lj        555\n",
      "ATMP_46025    227\n",
      "ATMP_lj       555\n",
      "DEWP_46025    227\n",
      "DEWP_lj       555\n",
      "DPD_46025     227\n",
      "DPD_lj        555\n",
      "GST_46025     227\n",
      "GST_lj        555\n",
      "MWD_46025     227\n",
      "MWD_lj        555\n",
      "PRES_46025    227\n",
      "PRES_lj       555\n",
      "TIDE_46025    227\n",
      "TIDE_lj       555\n",
      "VIS_46025     227\n",
      "VIS_lj        555\n",
      "WDIR_46025    227\n",
      "WDIR_lj       555\n",
      "WSPD_46025    227\n",
      "WSPD_lj       555\n",
      "WTMP_46025    227\n",
      "WTMP_lj       555\n",
      "WVHT_46025    227\n",
      "WVHT_lj       555\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(wind_data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_block(data, num_lags=1, tau=1):\n",
    "    ''' Get a dataframe with all the possible valid lags of the variables. '''\n",
    "    \n",
    "    backward_lags = pd.concat([data[var].shift(lag*tau).rename(f'{var}(t-{lag*tau})') for lag in range(num_lags+1) for var in data.columns], axis=1)\n",
    "    forward_lags  = pd.concat([data[var].shift(-1*lag*tau).rename(f'{var}(t+{lag*tau})') for lag in range(1,num_lags+1) for var in data.columns], axis=1)\n",
    "    block = pd.concat([backward_lags, forward_lags], axis=1)\n",
    "    \n",
    "    return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ccm(interaction, block, E_list, tau_list, theta_list, Tp, sample=50, sig=0.05):\n",
    "    #solver = HistGradientBoostingRegressor() #TRYING DIFFERNT SOLVER TO ENSURE CONVERGENCE\n",
    "    print(interaction)\n",
    "    lib = f'1 {len(block)}'\n",
    "    \n",
    "    # Get dataframe with two species of interest\n",
    "    A = interaction[0]; B = interaction[1]\n",
    "    df = block[[f'{A}(t-0)', f'{B}(t-0)']]\n",
    "    \n",
    "    driver = f'{A}(t-0)'\n",
    "    \n",
    "    E_tau_theta_results = pd.DataFrame(columns = ['E', 'tau', 'theta', 'rho'])\n",
    "    for E, tau, theta in list(product(E_list, tau_list, theta_list)):\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho = c.corr().iloc[0,1]\n",
    "        E_tau_theta_results.loc[len(E_tau_theta_results)] = [E, tau, theta, rho]\n",
    "    E_tau_theta_results = E_tau_theta_results.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "    # Assign E, tau, and theta to be the optimal E, tau, and theta\n",
    "    ccm_value = E_tau_theta_results['rho'].max()\n",
    "    E = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'E'].item())\n",
    "    tau = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'tau'].item())\n",
    "    theta = int(E_tau_theta_results.loc[np.where(E_tau_theta_results.rho==ccm_value),'theta'].item())\n",
    "        \n",
    "    # Get convergence p-value\n",
    "    convergence_p_value = get_convergence_p_value(block, sample, A, B, E, Tp, tau, theta)\n",
    "\n",
    "    # Preparing Output\n",
    "    output = {\n",
    "        'target (driver)': A,\n",
    "        'lib (driven)': B,\n",
    "        'E': E,\n",
    "        'tau': tau,\n",
    "        'theta': theta,\n",
    "        'E_tau_theta_results': E_tau_theta_results,\n",
    "        'ccm_value': ccm_value,\n",
    "        'convergence_p_value': convergence_p_value,\n",
    "        'correlation': df.corr().iloc[0,1]\n",
    "    }\n",
    "\n",
    "    return output\n",
    "\n",
    "def get_convergence_p_value(df, sample, A, B, E, Tp, tau, theta):\n",
    "    # Get convergence p-value for CCM (one-tailed t-test on cross-map values using 20% and 50% library sizes)\n",
    "    # H0: μ_20% ≥ μ_50%\n",
    "    # HA: μ_20% < μ_50%\n",
    "    # If p < 0.05, the 20% library size trials have a rho that is significantly smaller than the 50% library trials  \n",
    "    \n",
    "    libsize1 = int(np.ceil(df.shape[0]/5))   # 20% of the full library size\n",
    "    libsize2 = int(np.ceil(df.shape[0]/2))   # 50% of the full library size\n",
    "    \n",
    "    max_iterations = 10 * sample\n",
    "    \n",
    "    # Get list of rhos for libsize1\n",
    "    rhos1 = []; iteration_count = 0\n",
    "    while len(rhos1) < sample and iteration_count < max_iterations:\n",
    "        start = np.random.randint(libsize1, len(df))\n",
    "        library = [start - libsize1, start]\n",
    "        data_subset = df.iloc[library[0]:library[1]]\n",
    "        lib = f'{library[0]+1} {library[1]+1}'\n",
    "        driver = f'{A}(t-0)'\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho1 = c.corr().iloc[0,1]\n",
    "        if not np.isnan(rho1):\n",
    "            rhos1.append(rho1)\n",
    "        iteration_count += 1\n",
    "        \n",
    "    # Get list of rhos for libsize2\n",
    "    rhos2 = []; iteration_count = 0\n",
    "    while len(rhos2) < sample and iteration_count < max_iterations:\n",
    "        start = np.random.randint(libsize2, len(df))\n",
    "        library = [start - libsize2, start]\n",
    "        data_subset = df.iloc[library[0]:library[1]]\n",
    "        lib = f'{library[0]+1} {library[1]+1}'\n",
    "        driver = f'{A}(t-0)'\n",
    "        driven_embedded = [f'{B}(t{i})' if i < 0 else f'{B}(t-{i})' for i in range(E * tau, 1)]\n",
    "        driven_embedded = driven_embedded[::tau][:E]\n",
    "        c = SMap(dataFrame=block, target=driver, columns=driven_embedded, embedded=True, Tp=Tp, theta=theta, lib=lib, pred=lib, noTime=True)\n",
    "        c = c['predictions'][['Observations', 'Predictions']]\n",
    "        rho2 = c.corr().iloc[0,1]\n",
    "        if not np.isnan(rho2):\n",
    "            rhos2.append(rho2)\n",
    "        iteration_count += 1\n",
    "    \n",
    "    convergence_t_stat, convergence_p_value = ttest_ind(rhos1, rhos2, alternative='less')\n",
    "    \n",
    "    return convergence_p_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 50 interactions\n",
      "('GST_46025', 'WSPD_lj')\n",
      "('ATMP_46025', 'WSPD_lj')\n",
      "('GST_lj', 'WSPD_lj')\n",
      "('MWD_46025', 'WSPD_lj')\n",
      "('DPD_46025', 'WSPD_lj')\n",
      "('DEWP_lj', 'WSPD_lj')\n",
      "('DEWP_46025', 'WSPD_lj')\n",
      "('DPD_lj', 'WSPD_lj')\n",
      "('APD_lj', 'WSPD_lj')\n",
      "('APD_46025', 'WSPD_lj')\n",
      "('ATMP_lj', 'WSPD_lj')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m interaction \u001b[38;5;241m=\u001b[39m target_interactions[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThere are \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(target_interactions)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m interactions\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mccm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minteraction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mE_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtheta_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTp\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minteraction\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtarget_interactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(results)\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:2007\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   2001\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[1;32m   2002\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[1;32m   2003\u001b[0m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[1;32m   2004\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[1;32m   2005\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 2007\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:1650\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[0;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[0;32m-> 1650\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[1;32m   1652\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[1;32m   1653\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[1;32m   1654\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[1;32m   1655\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[1;32m   1656\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/pyedm_env/lib/python3.9/site-packages/joblib/parallel.py:1762\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1760\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[1;32m   1761\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1763\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[1;32m   1766\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[1;32m   1767\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "E_list = range(2,13)\n",
    "tau_list = [-1,-2,-3] #can try more taus\n",
    "theta_list = [0,0.1,0.5,1,2,3,4,5,6,7,8,9]\n",
    "Tp = 0\n",
    "exclusion_radius = 0\n",
    "\n",
    "all_ccm_results = pd.DataFrame()\n",
    "interactions = list(permutations(wind_data.columns.tolist(),2))\n",
    "target_interactions = [pair for pair in interactions if target in pair]\n",
    "\n",
    "interaction = target_interactions[0]\n",
    "print(f'There are {len(target_interactions)} interactions')\n",
    "\n",
    "results = Parallel(n_jobs=-1)(\n",
    "    delayed(ccm)(interaction, block, E_list, tau_list, theta_list, Tp) for interaction in target_interactions)\n",
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target (driver)</th>\n",
       "      <th>lib (driven)</th>\n",
       "      <th>E</th>\n",
       "      <th>tau</th>\n",
       "      <th>theta</th>\n",
       "      <th>ccm_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [target (driver), lib (driven), E, tau, theta, ccm_value]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system variables: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get CCM results that show convergence (convergence p-value < 0.05)\n",
    "\n",
    "ccm_cutoff = 0.5\n",
    "\n",
    "significant_results = results_df[results_df.convergence_p_value<0.05]\n",
    "significant_results = significant_results.sort_values(by='ccm_value', ascending=False)\n",
    "significant_results = significant_results[['target (driver)', 'lib (driven)', 'E', 'tau', 'theta', 'ccm_value']].reset_index(drop=True)\n",
    "\n",
    "display(significant_results[significant_results.ccm_value>ccm_cutoff])\n",
    "\n",
    "# Choose system variables where the CCM value to or from the target is > ccm_cutoff\n",
    "system_variables = significant_results[significant_results.ccm_value > ccm_cutoff]\n",
    "system_variables = system_variables[['target (driver)', 'lib (driven)']].values.flatten().tolist()\n",
    "system_variables = list(set(system_variables))\n",
    "print('system variables: ')\n",
    "display(sorted(system_variables))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Phosphate_(uM)(t-0)', 'Phosphate_(uM)(t-3)', 'Phosphate_(uM)(t+3)']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_valid_lags_tau(block, target, Tp, tau, num_lags, exclusion_radius, system_variables):\n",
    "    \n",
    "    # Get lags of system variables\n",
    "    system_variable_lags = []\n",
    "    for var in system_variables:\n",
    "        # Get forwards and backwards lag of the system variables\n",
    "        var_backwards_lags = [f'{var}(t{i})' if i < 0 else f'{var}(t-{i})' for i in range(num_lags * tau, 1)]\n",
    "        var_backwards_lags = var_backwards_lags[::tau][:num_lags]\n",
    "        var_forwards_lags  = [f'{var}(t+{i})' for i in range(-(num_lags-1) * tau + 1)]\n",
    "        var_forwards_lags  = var_forwards_lags[::tau][:num_lags-1]\n",
    "        var_lags = var_backwards_lags + var_forwards_lags\n",
    "        system_variable_lags = system_variable_lags + var_lags\n",
    "    \n",
    "    # Remove (t-0) lag of target variable from valid_lags\n",
    "    valid_lags = [x for x in system_variable_lags if x != f'{target}(t-0)']\n",
    "\n",
    "    # If Tp = 0, remove [-exclusion_radius, exclusion_radius] lags of target variable from valid lags\n",
    "    if Tp == 0:\n",
    "        for r in range(-exclusion_radius, exclusion_radius+1):\n",
    "            if r < 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t{r})']\n",
    "            elif r == 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t-{r})']\n",
    "            elif r > 0:\n",
    "                valid_lags = [x for x in valid_lags if x != f'{target}(t+{r})']\n",
    "                    \n",
    "    return valid_lags\n",
    "\n",
    "#target = 'Planktothrix_rubescens'\n",
    "system_variables = system_variables\n",
    "Tp = 0\n",
    "exclusion_radius = 6\n",
    "num_lags = 2   # Use -3, 0, and +3 lags of each variable\n",
    "tau = -3\n",
    "\n",
    "valid_lags = get_valid_lags_tau(block, target, Tp, tau, num_lags, exclusion_radius, system_variables)\n",
    "valid_lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E = 1, # embeddings = 3\n",
      "E = 2, # embeddings = 3\n"
     ]
    }
   ],
   "source": [
    "random_embeddings = {}\n",
    "for E in range(1,3):\n",
    "    # Get random embeddings using valid lags\n",
    "    embeddings = set()\n",
    "    sample = 3#100000\n",
    "    max_trials = 5#10000000\n",
    "    trials = 0\n",
    "    while len(embeddings) < sample and trials < max_trials:\n",
    "        embedding = tuple(random.sample(valid_lags, E))\n",
    "        sorted_embedding = tuple(sorted(embedding))\n",
    "        if sorted_embedding not in embeddings:\n",
    "            embeddings.add(sorted_embedding)\n",
    "    trials += 1\n",
    "    embeddings = [list(embedding) for embedding in embeddings]\n",
    "    random_embeddings['{0}'.format((target, E, Tp, exclusion_radius))] = embeddings\n",
    "    print(f'E = {E}, # embeddings = {len(embeddings)}')\n",
    "    \n",
    "with open('random_embeddings_HAB.pkl', 'wb') as file:\n",
    "     pickle.dump(random_embeddings, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"('Avg_Chloro_(mg/m3)', 1, 0, 0)\": [['Phosphate_(uM)(t+3)'],\n",
       "  ['Phosphate_(uM)(t-0)'],\n",
       "  ['Phosphate_(uM)(t-3)']],\n",
       " \"('Avg_Chloro_(mg/m3)', 2, 0, 0)\": [['Phosphate_(uM)(t-0)',\n",
       "   'Phosphate_(uM)(t-3)'],\n",
       "  ['Phosphate_(uM)(t+3)', 'Phosphate_(uM)(t-3)'],\n",
       "  ['Phosphate_(uM)(t+3)', 'Phosphate_(uM)(t-0)']]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load HAB random embeddings\n",
    "with open('random_embeddings_HAB.pkl', 'rb') as file:\n",
    "    HAB_embeddings = pickle.load(file)\n",
    "HAB_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create folder to store xmap results\n",
    "folder = 'xmap results HAB 100000 random embeddings'\n",
    "if not os.path.exists(folder):\n",
    "    os.makedirs(folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/147 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m key \u001b[38;5;241m=\u001b[39m [key \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m HAB_embeddings\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m target \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m E \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m==\u001b[39m Tp \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28meval\u001b[39m(key)[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m==\u001b[39m exclusion_radius]\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(key)\n\u001b[0;32m---> 22\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m HAB_embeddings[\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m]\n\u001b[1;32m     24\u001b[0m xmap_results \u001b[38;5;241m=\u001b[39m get_xmap_results_smap(block, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m(t-0)\u001b[39m\u001b[38;5;124m'\u001b[39m, embeddings, Tp, theta, lib, lib)\n\u001b[1;32m     26\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mxmap_results_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Tp_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_E_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_theta_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtheta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# Save HAB cross-mapping results\n",
    "\n",
    "E_list = range(4,25)\n",
    "theta_list = [1,5,9,15,25,35,45]\n",
    "Tp = 0\n",
    "exclusion_radius = 6\n",
    "self_weight = 0  # self_weight = 0 for gap filling\n",
    "lib = '1 832' #CHANGE TO LIBRARY SIZE\n",
    "pred = '1 832'\n",
    "\n",
    "total_iterations = len(E_list) * len(theta_list)\n",
    "\n",
    "gapfill_results = {}\n",
    "parameters = pd.DataFrame(columns=['target', 'noise_level', 'lib', 'pred', 'E', 'theta', 'k', 'rho'])\n",
    "block = get_block(HAB_data, num_lags=50)\n",
    "\n",
    "with tqdm(total=total_iterations) as pbar:\n",
    "    for E, theta in product(E_list, theta_list):\n",
    "\n",
    "        key = [key for key in HAB_embeddings.keys() if eval(key)[0] == target and eval(key)[1] == E and eval(key)[2] == Tp and eval(key)[3] == exclusion_radius]\n",
    "        print(key)\n",
    "        embeddings = HAB_embeddings[key[0]]\n",
    "\n",
    "        xmap_results = get_xmap_results_smap(block, f'{target}(t-0)', embeddings, Tp, theta, lib, lib)\n",
    "\n",
    "        file_path = os.path.join(folder, f'xmap_results_{target}_Tp_{Tp}_E_{E}_theta_{theta}.pkl')\n",
    "\n",
    "        with open(file_path, 'wb') as f:\n",
    "            pickle.dump(xmap_results, f)\n",
    "\n",
    "        pbar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([\"('Avg_Chloro_(mg/m3)', 1, 0, 0)\", \"('Avg_Chloro_(mg/m3)', 2, 0, 0)\"])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HAB_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyedm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
